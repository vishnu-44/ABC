# ABC
Attribution-Based Confidence Metric For  Deep Neural Networks
The paper introduces a novel confidence metric for deep neural networks (DNNs) called **Attribution-Based Confidence (ABC)**. This metric aims to assess whether the output of a DNN on a given input can be trusted. DNNs are often vulnerable to adversarial attacks and struggle with inputs outside the training distribution, but there is a lack of reliable measures of model confidence that align well with the model's accuracy. The proposed ABC metric addresses these challenges by being computationally effective and not requiring access to training data, ensembles, or a separate calibration model.

The metric is useful even when only the trained model is available for inference. The paper mathematically motivates the ABC metric and evaluates its performance through two key experiments. The first examines how the model's accuracy and associated confidence change with out-of-distribution inputs. The second tests the metric's effectiveness in detecting adversarial attacks like FGSM, CW, DeepFool, PGD, and adversarial patches. The results show that the ABC metric is low for both out-of-distribution data and adversarial examples, correlating with lower model accuracy, and demonstrating its potential to improve the trustworthiness and resilience of DNNs.
